\documentclass[12pt,preprint]{aastex}

\newcounter{address}
\setcounter{address}{0}
\newcommand{\foreign}[1]{\textit{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\LCOGT}{\project{LCOGT}}
\newcommand{\documentname}{\textsl{Article}}

\newcommand{\given}{\,|\,}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}

\title{DRAFT: \\
  Lucky imaging without the terrible, \emph{terrible} waste \\
  \textit{or} \\
  Don't throw away data!}
\author{
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA},\ref{email}},
  Daniel~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  Federica Bianco\altaffilmark{\ref{CCPP},\ref{LCOGT}}
}

\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP} Center
  for Cosmology and Particle Physics, Department of Physics, New York
  University, 4 Washington Place, New York, NY 10003}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}
  Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117
  Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email} To whom
  correspondence should be addressed: \texttt{david.hogg@nyu.edu}}
\altaffiltext{\theaddress}{\stepcounter{address}\label{LCOGT}
  \LCOGT, UCSB, or whatever}

\begin{abstract}
We present a new method---and a new data-processing pipeline---for
working with Lucky Imaging data, in which high-resolution images are
obtained by combining very large numbers of short exposures under a
variable point-spread-function-distorting atmosphere.  The method is
based on previous work on multi-frame deconvolution; it is valuable
because it does \emph{not} involve discarding any data whatsoever.
The method works by extracting information about the scene from every
image, no matter what that image's PSF.  We demonstrate the method on
data taken with the \LCOGT\ lucky imaging system.
\end{abstract}

\begin{document}

\textbf{[This document is a very early draft dated 2012-04-23.  Please do not cite it without the permission of the authors.]}

\section{Introduction}

...What is lucky imaging traditionally and what are the benefits and costs?..

...What is speckle imaging and it's poor cousin shift-and-add?..

...Why is it that a Real Scientist (TM) commits to Never Throw Away Data (TM)?..

\section{Imaging generalities}

From our perspective, an image read out by a real camera (with, say,
square pixels in a focal-plane array) is a noisy sampling of the
intensity field, convolved with some kind of point-spread function.
Importantly, if you want to think of the image as a pure
\emph{sampling} of a convolved intensity field---and trust us, you
do---then the point-spread function that convolves the intensity field
should be not the pure atmospheric PSF, nor even the
instrument-convolved atmospheric PSF.  It should be the
\emph{pixel-convoved} PSF \emph{at the focal plane}.  From here on,
whenever we mention or use the PSF, we mean \emph{always} the
pixel-convolved PSF at the focal plane.  This choice may seem strange,
but when this pixel-convolved PSF is used, the pixel values are
delta-function samples of the convolved intensity field, reducing
enormously synthetic-image computation.  Furthermore, if the
non-pixel-convolved PSF is smooth and well-sampled, the
pixel-conovolved PSF is \emph{also} smooth, so there are no significant
numerical losses or approximations incurred by making this choice.

It pains us to point out that the PSF, as it is usually conceived, is
actually \emph{correlated} not \emph{convolved} with the true scene.
That is a choice about what the PSF is.  In what follows we will use
the word ``convolve'' in conformity with usual practice in astronomy,
but the way we show the PSFs in the figures, it is probably
``correlate'' that we are really doing.

To make our ideas about imaging concrete, we can represent the model
that will be used throughout this paper for imaging data as a
convolution
\begin{eqnarray}\displaystyle
D_n &=& \psi_n \ast I + E_n
\quad ,
\end{eqnarray}
where $D_n$ is the data image---one of $N$ noisy $M$-pixel
images---with index $1<n<N$, $\psi_n$ is the pixel-convolved PSF
appropriate for image $n$, $\ast$ represents the convolution
operation, $I$ is the intensity field above the atmosphere, and $E_n$
is the noise contribution to image $n$.  Convolution is a linear
operation, so, following \citet{hirsch}, we can write the model in two
equivalent ways:
\begin{eqnarray}\displaystyle
d_n &=& P_n \cdot s + e_n
\\
d_n &=& S \cdot p_n + e_n
\quad ,
\end{eqnarray}
where now $d_n$ is the original data image $D_n$ reformatted as a
one-dimensional length-$M$ column vector, $P_n$ is a $W\times M$
sparse matrix that contains $K$ independent values representing the
point-spread function for image $n$ on a frighteningly reformatted
pixel grid, $s$ is a length-$W$ column vector representing the true
scene, $e_n$ is a length-$M$ column vector of noise constributions to
image $n$, $S$ is a $K\times M$ sparse matrix that contains $W$
independent values representing the true scene also frighteningly
reformatted, and $p_n$ is a length-$K$ column vector representing the
PSF.  The idea is that if the image data are unwrapped into a
one-dimensional vector, the linear convolution operation can always be
represented as a matrix operation acting on another vector, and there
is a choice of whether to see the PSF as the matrix and the scene as
the vector, or vice versa.  Not obvious?  Think about it!

...insert here a figure demonstrating the model for one of the \LCOGT\ images...

In real astronomical applications, neither the PSF nor the true scene
is known \foreign{a priori}.  If our interest is the true scene---and
it usually is---then astronomy is a special case of what's known in
the computer science literature as \emph{blind deconvolution}.  We
want to explain the data as being produced by something fundamental of
interest convolved with instrumental resolution that is not known in
advance and also of no particular interest in itself.  Astronomers are
loath to use the word ``deconvolution'' but any time we make a catalog
of precise stellar positions and fluxes from noisy, low-resolution
data, we are performing some kind of deconvolution: We have
information about the positions of stars that is much more precise
than the the angular precision of any instrument-realized PSF.

One small but perhaps not insignificant issue with the model expressed
here is that it posits the existence of a ``true scene''.  For
technical reasons---related to the finiteness of any real data
stream---it is better to think of this ``true image'' as really an
instrument for making finite-resolution predictions.  It is not going
to be an accurate representation of the scene we would see with an
implausible infinitely large telescope!  It is a representation of the
intensity field that is only to be used within the context of
covolution with a finite PSF.  Similar issues arise in radio astronomy
when interferometric data are ``cleaned''.

Now, with an error model---a probabilistic description of how the
$E_n$ image (or equivalently, $e_n$ vector) is generated---we can
write down a likelihood function or a probability for the data $D_n$
(or equivalently $d_n$) given the model parameters $p_n$ and $s$.  For
example, if the $M$ per-pixel noise contributions are (independently)
drawn from Gaussians with zero means then this likelihood has the
incredibly simple form
\begin{eqnarray}\displaystyle
\ln p(D_n\given p_n, s) &=& Q - \frac{1}{2}\,\chi^2_n
\\
\chi^2_n &\equiv& \transpose{[d_n - P_n \cdot s]} \cdot C^{-1} \cdot [d_n - P_n \cdot s]
\quad ,
\end{eqnarray}
where $Q$ is some constant, we have used the column-vectoriness of
$d_n$, sparse matrix $P_n$ is trivially constructable from $p_n$, and
$C^{-1}$ is a diagonal $M\times M$ matrix with per-pixel inverse
variances on the diagonal and zeros everywhere else; this gives
$\chi^2_n$ the standard ``chi-squared'' meaning.  (The data pixels can
be made non-independent by adding off-diagonal terms to $C^{-1}$.)

With some priors, we can even in principle write down a posterior
probability distribution function over true scenes $s$ and PSFs $p_n$
given the data (the set of all $N$ images $D_n$ or vectors $d_n$).
This posterior PDF can even be \emph{marginalized} over all possible
PSFs to leave us with a marginalized posterior PDF for just the true
scene $s$.  That would be just about the best we could possibly do in
this problem, for very general reasons.

Unfortunately, even if we apply unrealistically restrictive priors on
image space, any posterior PDF over true scenes $s$ will be a
dimensionally immense object.  Even when, in the experiments that
follow, we restrict to small image patches, any useful description of
``scene space'' will have a dimensionality $>10^4$ (the number of
pixels in the scene representation).  Perhaps someday soon we will
have good ways of describing non-trivial PDFs in spaces like
this---maybe we even already do---but we (the authors) don't know
anything realistic at the present day.  So proper probabilistic
inference is not an option in the short term.  The posterior PDF over
PSFs is even worse, because there is a different PSF for every image,
and in the experiments that follow we will be using $>10^3$ images,
each of which has a $10^{2.5}$-parameter PSF.

These considerations lead us to optimization rather than sampling or
full probabilistic approaches.  What to optimize?

...How regularization and optimization is related to the prior PDF and posterior PDF...

...Some general discussion of regularizations and their consequences.
Spend some lines bashing non-negative on $s$.  Why is that so
dangerous?..

...How, in general, are we going to proceed; why is online useful...

\section{The method and pipeline}

\section{Experiments and results}

\section{Discussion}

\acknowledgements It is a pleasure to thank Stefan Harmeling
(T\"ubingen), Michael Hirsch (UCL), and Bernhard Sch\"olkopf
(T\"ubingen) for introducing us to the ideas in \citet{hirsch}; many
of the technical ideas in this \documentname\ come from their work.
We also single out Dustin Lang (CMU) and Robert Lupton (Princeton) for
special thanks; DWH wouldn't understand anything about imaging if it
weren't for the many years of discussions.  DWH and DFM were partially
supported by NASA (grant GRANT NUMBER HERE) and the NSF (grant
IIS-1124794).  This project made use of the NASA \project{Astrophysics
  Data System}, and code in the the \project{numpy}, \project{scipy},
and \project{matplotlib} open-source projects.  All the code used in
this paper is available at [URL HERE], and sample data are available
at [URL HERE].

\begin{thebibliography}{70}
\bibitem[Hirsch \etal(2011)]{hirsch}
Hirsch,~M., Harmeling,~S., Sra,~S., Sch\"olkopf,~B., 2011, \aap, 531, A9
\end{thebibliography}

\end{document}
