\documentclass[12pt,preprint]{aastex}

% has to be before amssymb it seems
\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}

\usepackage{url}
\usepackage{algorithmic,algorithm}
\usepackage{amssymb,amsmath}

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}

\newcommand{\project}[1]{{\sffamily #1}}
\newcommand{\TheThresher}{\project{The~Thresher}}
\newcommand{\LCOGT}{\project{LCOGT}}
\newcommand{\Python}{\project{Python}}
\newcommand{\numpy}{\project{numpy}}
\newcommand{\github}{\project{GitHub}}
\newcommand{\pip}{\project{pip}}
\newcommand{\paper}{\emph{Article}}
\newcommand{\license}{GNU General Public License v2}

\newcommand{\documentname}{\textsl{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}
\newcommand{\Algo}[1]{Algorithm~\ref{algo:#1}}
\newcommand{\algo}[1]{\Algo{#1}}
\newcommand{\algolabel}[1]{\label{algo:#1}}

% math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\bvec}[1]{\boldsymbol{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\unit}[1]{\,\mathrm{#1}}

% Parameters
\newcommand{\data}{\ensuremath{D}}
\newcommand{\scene}{\ensuremath{\Sigma}}
\newcommand{\psf}{\ensuremath{\psi}}
\newcommand{\dpsf}{\ensuremath{\tilde{\psi}}}
\newcommand{\dvec}{\ensuremath{d}}
\newcommand{\evec}{\ensuremath{e}}
\newcommand{\svec}{\ensuremath{s}}
\newcommand{\smat}{\ensuremath{S}}
\newcommand{\pvec}{\ensuremath{p}}
\newcommand{\pmat}{\ensuremath{P}}

\newcommand{\dfmplot}[1]{%
\begin{center}%
    \includegraphics[width=\textwidth]{#1}%
\end{center}%
}

\begin{document}

\title{\TheThresher: We don't throw away data}

\author{%
    Daniel~Foreman-Mackey\altaffilmark{\ref{CCPP},\ref{email}} \&
    David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA}}
}

\newcounter{address}
\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP} Center
  for Cosmology and Particle Physics, Department of Physics, New York
  University, 4 Washington Place, New York, NY 10003}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email} To whom
  correspondence should be addressed: \texttt{danfm@nyu.edu}}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}
  Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117
  Heidelberg, Germany}

\begin{abstract}
    Source code available at \url{http://danfm.ca/thresher}
    under the \license.
\end{abstract}

\keywords{%
    methods: data analysis ---
    methods: numerical ---
    methods: statistical
}

\clearpage

\section{Introduction}

High-resolution imaging has seen great success in the last decades,
with the \project{Hubble Space Telescope}, natural and artificial
guide-star adaptive optics, and various post-processing techniques.
With ground-based telescopes, very sophisticated hardware and/or
software is required if one hopes to obtain high angular resolution.
This is because atmospheric turbulence causes high frequency variations
in the point spread function (PSF) that quickly degrade the angular
resolution of the optics. The PSF of a standard long exposure suitable
for astronomy will be smooth and much wider than the ideal diffraction
limit. This is because it is produced by the time average over
atmospheric variations. The standard post-processing techniques for
dealing with this involve obtaining a large number of extremely short
exposures---on the timescale of the atmospheric variations. These
short exposures have low signal-to-noise and tend to have very
complicated PSFs but they provide strong constraints on the
diffraction limited scene when many images are used together.

The idea of exploiting the high resolution information in short
exposures was first suggested by \citet{labeyrie} and there has
been a rich literature on this topic since then. In particular,
a very popular technique called \emph{traditional lucky imaging}
\citep[TLI;][]{law} is based on Fried's derivation of the probability
of obtaining a
frame with unusually ``lucky'' seeing when many images are taken
quickly. That is, occasionally---just by chance---the wavefront
distortions from the atmosphere will come close to canceling (or
really canceling the imperfections in your real telescope).
Lucky imaging is very popular because it is simple to understand and
implement and it can be used with very inexpensive equipment, but as a
technique it is not inexpensive because it results in a substantial
amount of wasted data.

In a typical TLI imaging run, you take an enormous number
of short frames and \emph{throw almost all of them away}.  The
resulting average, stack, or co-add of the best images can have very
high angular resolution.  It doesn't, of course, have very high
signal-to-noise, simply because so little data in the end get used
(typically about one percent), but it can be very good for making
measurements that require high spatial resolution.

Lucky imaging is straightforward---and very simple to implement---but
it violates one very important principle of empirical science: there
is no way that \emph{throwing away} non-corrupted data can give you
\emph{more information}.  It might be that doing something heuristic
with bad data will make your results worse, but if you are doing
something probabilistically justifiable, no datum can be
\emph{negatively informative}.  The worst a datum can be is useless.
There can't ever be an inferential justification---other than
pragmatic---for discarding any relevant data.

In the case of lucky imaging, the fundamental reason that throwing
away data helps is that the core data analysis step is co-addition of
the imaging, which (though widely used in astronomy) is not
justifiable when the point-spread function is varying rapidly.  Lucky
imaging is also very sensible operationally: It provides results
rapidly (even on-the-fly during the observing run), and permits
analysis of enormous data sets without enormous computation (for
example, it can be implemented in map-reduce). The method that we
propose here is not as fast or as computationally efficient.  It is,
however, justifiable within probabilistic data analysis and will
make use of every datum.

\TheThresher\ is an alternative to TLI and other post-processing
techniques that is a special case of a broad class of algorithms
called \emph{blind deconvolution} \citep{ayers}. In particular, it
is an extension of the multi-frame online blind deconvolution (MOBD)
from \citet{hirsch}. In this \documentname, we describe the theory
behind and implementation of the robust blind deconvolution engine
behind \TheThresher. We also demonstrate the significant improvements
made possible by this technique using both simulated and real data.

\section{Imaging} \sectlabel{imaging}

From our perspective, an image read out by a real camera (with, say,
square pixels in a focal-plane array) is a noisy sampling of the
intensity field, convolved with some kind of point-spread function.
Importantly, if you want to think of the image as a pure
\emph{sampling} of a convolved intensity field---and trust us, you
do---then the point-spread function that convolves the intensity field
should be not the pure atmospheric PSF, nor even the
instrument-convolved atmospheric PSF.  It should be the
\emph{pixel-convolved} PSF \emph{at the focal plane}.  From here on,
whenever we mention or use the PSF, we mean \emph{always} the
pixel-convolved PSF at the focal plane.  This choice may seem strange,
but when this pixel-convolved PSF is used, the pixel values are
delta-function samples of the convolved intensity field, reducing
enormously synthetic-image computation.  Furthermore, if the
non-pixel-convolved PSF is smooth and well-sampled, the
pixel-convolved PSF is \emph{also} smooth, so there are no significant
numerical losses or approximations incurred by making this choice.

% % Maybe I don't get it but this seems not totally needed...
% It pains us to point out that the PSF, as it is usually conceived, is
% actually \emph{correlated} not \emph{convolved} with the true scene.
% That is a choice about what the PSF is.  In what follows we will use
% the word ``convolve'' in conformity with usual practice in astronomy,
% but the way we show the PSFs in the figures, it is probably
% ``correlate'' that we are really doing.

To make our ideas about imaging concrete, we can represent the model
that will be used throughout this paper for imaging data as a
convolution
\begin{equation}\eqlabel{convimg}
    \data_n = \psf_n \ast \scene + E_n \quad,
\end{equation}
where $\data_n$ is the data image---one of $N$ noisy $M$-pixel
images---with index $1<n<N$, $\psf_n$ is the pixel-convolved PSF
appropriate for image $n$, $\ast$ represents the convolution
operation, $\scene$ is the ``true'' intensity field (``scene'') above
the atmosphere, and $E_n$ is the noise contribution to image $n$.
Since convolution is a linear operation, it can be written in matrix
form. This is especially easy to visualize in this context where all of
the convolutions are discrete. Casting \eq{convimg}---our
\emph{generative model} for an astronomical image---as a linear system
has significant computational benefits and there are many techniques
available for solving such an system. Furthermore, following
\citet{hirsch}, we can write the model in two equivalent ways:
\begin{eqnarray}\eqlabel{linimg}\displaystyle
\dvec_n &=& \pmat_n \cdot \svec + \evec_n
\\
\dvec_n &=& \smat \cdot \pvec_n + \evec_n
\quad ,
\end{eqnarray}
where now $\dvec_n$ is the original data image $\data_n$ reformatted as a
one-dimensional length-$M$ column vector, $\pmat_n$ is a $W\times M$
sparse matrix that contains $K$ independent values given by the entries
in $\psf_n$, $s$ is a length-$W$ column vector representing the true
scene, $\evec_n$ is a length-$M$ column vector of noise contributions to
image $n$, $\smat$ is a $K\times M$ dense matrix that contains $W$
independent values representing the scene, and $\pvec_n$ is a length-$K$
column vector representing the PSF. In \eq{linimg}, the transformations
$\psf_n \to \pmat_n$ and $\scene \to \smat$ are illustrated in \fig{index}.

\begin{figure}[!htbp]
    \dfmplot{index_gymnastics.pdf}
    \caption{Demonstration of the index gymnastics needed to convert between
        images and the matrix representation of convolution.\figlabel{index}}
\end{figure}

The idea is that if the image data are unwrapped into a
one-dimensional vector, the linear convolution operation can always be
represented as a matrix operation acting on another vector, and there
is a choice of whether to see the PSF as the matrix and the scene as
the vector, or vice versa.  Not obvious?  Think about it!

In real astronomical applications, neither the PSF nor the true scene
is known \foreign{a priori}.  If our interest is the true scene---and
it usually is---then astronomy is a special case of what's known in
the computer science literature as \emph{blind deconvolution}.  We
want to explain the data as being produced by something fundamental of
interest convolved with instrumental resolution that is not known in
advance and also of no particular interest in itself.  Astronomers are
loath to use the word ``deconvolution'' but any time we make a catalog
of precise stellar positions and fluxes from noisy, low-resolution
data, we are performing some kind of deconvolution: We have
information about the positions of stars that is much more precise
than the the angular precision of any instrument-realized PSF.

One small but perhaps not insignificant issue with the model expressed
here is that it posits the existence of a ``true scene''.  For
technical reasons---related to the finiteness of any real data
stream---it is better to think of this ``true image'' as really an
instrument for making finite-resolution predictions.  It is not going
to be an accurate representation of the scene we would see with an
implausible infinitely large telescope!  It is a representation of the
intensity field that is only to be used within the context of
covolution with a finite PSF.  Similar issues arise in radio astronomy
when interferometric data are ``cleaned''.

Now, with an error model---a probabilistic description of how the
$E_n$ image (or equivalently, $e_n$ vector) is generated---we can
write down a likelihood function or a probability for the data $\data_n$
(or equivalently $d_n$) given the model parameters $p_n$ and $s$.  For
example, if the $M$ per-pixel noise contributions are (independently)
drawn from Gaussians with zero means then this likelihood has the
incredibly simple form
\begin{eqnarray}\displaystyle
\ln p(\data_n\given p_n, s) &=& Q - \frac{1}{2}\,\chi^2_n
\\
\chi^2_n &\equiv& \transpose{[d_n - P_n \cdot s]} \cdot C^{-1} \cdot [d_n - P_n \cdot s]
\quad ,
\end{eqnarray}
where $Q$ is some constant, we have used the column-vectoriness of
$d_n$, sparse matrix $P_n$ is trivially constructable from $p_n$, and
$C^{-1}$ is a diagonal $M\times M$ matrix with per-pixel inverse
variances on the diagonal and zeros everywhere else; this gives
$\chi^2_n$ the standard ``chi-squared'' meaning.  (The data pixels can
be made non-independent by adding off-diagonal terms to $C^{-1}$.)

With some priors, we can even in principle write down a posterior
probability distribution function over true scenes $s$ and PSFs $p_n$
given the data (the set of all $N$ images $\data_n$ or vectors $d_n$).
This posterior PDF can even be \emph{marginalized} over all possible
PSFs to leave us with a marginalized posterior PDF for just the true
scene $s$.  That would be just about the best we could possibly do in
this problem, for very general reasons.

Unfortunately, even if we apply unrealistically restrictive priors on
image space, any posterior PDF over true scenes $s$ will be a
dimensionally immense object.  Even when, in the experiments that
follow, we restrict to small image patches, any useful description of
``scene space'' will have a dimensionality $>10^4$ (the number of
pixels in the scene representation).  Perhaps someday soon we will
have good ways of describing non-trivial PDFs in spaces like
this---maybe we even already do---but we (the authors) don't know
anything realistic at the present day.  So proper probabilistic
inference is not an option in the short term.  The posterior PDF over
PSFs is even worse, because there is a different PSF for every image,
and in the experiments that follow we will be using $>10^3$ images,
each of which has a $10^{2.5}$-parameter PSF.

These considerations lead us to optimization rather than sampling or
full probabilistic approaches.  What to optimize?

...How regularization and optimization is related to the prior PDF and posterior PDF...

...Some general discussion of regularizations and their consequences.
Spend some lines bashing non-negative on $s$.  Why is that so
dangerous?..

...How, in general, are we going to proceed; why is online useful...

\section{Blind Deconvolution}

To understand the process of blind deconvolution (BD), we must first discuss
what an image is. It is standard practice to describe an observed image
$D_n$ as the convolution of a constant scene $S$ with a point-spread function
(PSF) $\Psi_n$ plus a noise contribution $E_n$
\begin{equation} \eqlabel{full-convol}
    D_n = \Psi_n \ast S + E_n \quad.
\end{equation}
The PSF $\Psi_n$ is the \emph{pixel-convolved} PSF at the focal plane of the
detector at the time of the observation. It will include effects due to
the atmosphere and the optics themselves. In general, the true forms of
$\Psi_n$, $S$ and $E_n$ in \eq{full-convol} are completely \emph{unknown}
and we would like to infer both $\Psi_n$ and $S$.

Following \citet{magain}, it is useful to introduce a smoothing kernel $K$
(with a size that is compatible with the sampling of the instrument)
such that
\begin{equation}
    \Psi_n = K \ast P_n
\end{equation}
where $P_n$ is the \emph{deconvolved}-PSF (DPSF). \Eq{full-convol} can then
be rewritten as
\begin{equation} \eqlabel{soft-convol}
    D_n = K \ast P_n \ast S + E_n \quad.
\end{equation}
With the correct re-shaping (to be explained), this operation can be
re-written as a matrix multiplication in two equivalent ways
\begin{eqnarray}
    d_n = P_n^\prime \cdot s + e_n \quad \mathrm{and} \\
    d_n = S^\prime \cdot p_n + e_n
\end{eqnarray}
where $s = (K \ast S)^\prime$ and $p = \Psi_n^\prime$.


\section{Implementation}

\begin{thebibliography}{}\raggedright

\bibitem[Ayers\ \&\ Dainty(1988)]{ayers}
    Ayers, G. R., \& Dainty, J. C. 1988, Opt. Lett., 13, 547

\bibitem[Fried(1978)]{fried}
    Fried, D. L. 1978, J. Opt. Soc. Amer., 86

\bibitem[Hirsch\ \etal(2011)]{hirsch}
Hirsch, M., Harmeling, S., Sra, S., \& Sch{\"o}lkopf, B.\ 2011, \aap, 531, A9

\bibitem[Labeyrie(1970)]{labeyrie} Labeyrie, A.\ 1970, \aap, 6, 85

\bibitem[Law\ \etal(2006)]{law}
    Law, N.~M., Mackay, C.~D., \& Baldwin, J.~E.\ 2006, \aap, 446, 739

\bibitem[Magain\ \etal(1998)]{magain} Magain, P., Courbin, F.,
    \& Sohy, S.\ 1998, \apj, 494, 472



\end{thebibliography}

\clearpage
\appendix

\end{document}
