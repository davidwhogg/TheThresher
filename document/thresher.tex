% This file is part of The Thresher.

% to-do
% -----
% - Adam Bolton commented on my blog about http://arxiv.org/abs/0911.2689 and http://arxiv.org/abs/1111.6525 
% - write first draft
% - make and insert figures
% - send to Lauer, Marshall, Kendrew, Brandner, Crotts for comments
% - will require a full notation audit!

\documentclass[12pt,preprint]{aastex}

\newcounter{address}
\setcounter{address}{0}
\newcommand{\foreign}[1]{\textit{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\TheThresher}{\project{The~Thresher}}
\newcommand{\LCOGT}{\project{LCOGT}}
\newcommand{\documentname}{\textsl{Article}}

\newcommand{\given}{\,|\,}
\newcommand{\transpose}[1]{{#1}^{\mathsf{T}}}
\newcommand{\data}{D}
\newcommand{\scene}{S}
\newcommand{\psf}{\psi}
\newcommand{\dpsf}{\tilde{\psi}}

\title{DRAFT: \TheThresher: \\
  Lucky imaging without the terrible, \emph{terrible} waste \\
  \textit{or} \\
  Don't throw away data!}
\author{
  David~W.~Hogg\altaffilmark{\ref{CCPP},\ref{MPIA},\ref{email}},
  Daniel~Foreman-Mackey\altaffilmark{\ref{CCPP}},
  Federica Bianco\altaffilmark{\ref{CCPP},\ref{LCOGT}}
}

\setcounter{address}{1}
\altaffiltext{\theaddress}{\stepcounter{address}\label{CCPP} Center
  for Cosmology and Particle Physics, Department of Physics, New York
  University, 4 Washington Place, New York, NY 10003}
\altaffiltext{\theaddress}{\stepcounter{address}\label{MPIA}
  Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117
  Heidelberg, Germany}
\altaffiltext{\theaddress}{\stepcounter{address}\label{email} To whom
  correspondence should be addressed: \texttt{david.hogg@nyu.edu}}
\altaffiltext{\theaddress}{\stepcounter{address}\label{LCOGT}
  \LCOGT, UCSB, or whatever}

\begin{abstract}
We present a new method---and a new data-processing pipeline---for
working with Lucky Imaging data, in which high-resolution images are
obtained by combining very large numbers of short exposures taken
under a variable point-spread function.  The method is based on
previous work on multi-frame deconvolution; it is valuable because it
does \emph{not} involve discarding any data whatsoever.  The method
works by extracting information about the high-resolution scene from
every image, no matter what that image's PSF.  We demonstrate the
method on data taken with the \LCOGT\ lucky imaging system.
\end{abstract}

\begin{document}

\textbf{[This document is a very early draft dated 2012-05-25.  Please do not cite it without the permission of the authors.]}

\section{Introduction}

High-resolution imaging has seen great success in the last decades,
with the \project{Hubble Space Telescope}, natural and artificial
guide-star adaptive optics, and lucky imaging.  Lucky imaging is
possible with very inexpensive equipment, but as a technique it is not
inexpensive because it involves so much waste.

The idea behind traditional lucky imaging is that typical, smooth
atmospheric seeing is a time average of a rapidly varying, multi-modal
point-spread function.  If you take images rapidly with a very fast
camera, occasionally the atmosphere will---just through
chance---produce a nearly diffraction-limited image.  That is,
occasionally the wavefront distortions from the atmosphere will come
close to canceling (or really cancelling the imperfections in your
real telescope).  In a lucky imaging run, you take an enormous number
of short frames and \emph{throw almost all of them away}.  The
resulting average, stack, or co-add of the best images can have very
high angular resolution.  It can't have very high signal-to-noise,
simply because so little data in the end get used (typically much less
than one percent), but it can be very good for making measurements
that require high spatial resolution.

Lucky imaging is straightforward---and very simple to implement---but
it violates one very important principle of empirical science: There
is no way that \emph{throwing away} non-corrupted data can give you
\emph{more information}.  It might be that doing something heuristic
with bad data will make your results worse, but if you are doing
something probabilistically justifiable, no datum can be
\emph{negatively informative}.  The worst a datum can be is useless.
There can't ever be an inferential justification---other than
pragmatic---for discarding any relevant data.

In the case of lucky imaging, the fundamental reason that throwing
away data helps is that the core data analysis step is co-addition of
the imaging, which (though widely used in astronomy) is not
justifiable when the point-spread function is varying rapidly.  Lucky
imaging is also very sensible operationally: It provides results
rapidly (even on-the-fly during the observing run), and permits
analysis of enormous data sets without enormous computation (for
example, it can be implemented in map-reduce).  What we are about to
propose will not be as fast or as computationally efficient.  It will,
however, be justifiable within probabilistic data analysis and will
make use of every datum.

...What is speckle imaging and it's poor cousin shift-and-add?..

...Do a demo of traditional lucky imaging (TLI) where we move the
threshold and see the results change...

...How are these ideas related to deconvolution \citep{hirsch} and why
do we want band-limited deconvolution \citep{magain}?..

\section{Imaging generalities}

From our perspective, an image read out by a real camera (with, say,
square pixels in a focal-plane array) is a noisy sampling of the
intensity field, convolved with some kind of point-spread function.
Importantly, if you want to think of the image as a pure
\emph{sampling} of a convolved intensity field---and trust us, you
do---then the point-spread function that convolves the intensity field
should be not the pure atmospheric PSF, nor even the
instrument-convolved atmospheric PSF.  It should be the
\emph{pixel-convoved} PSF \emph{at the focal plane}.  From here on,
whenever we mention or use the PSF, we mean \emph{always} the
pixel-convolved PSF at the focal plane.  This choice may seem strange,
but when this pixel-convolved PSF is used, the pixel values are
delta-function samples of the convolved intensity field, reducing
enormously synthetic-image computation.  Furthermore, if the
non-pixel-convolved PSF is smooth and well-sampled, the
pixel-conovolved PSF is \emph{also} smooth, so there are no significant
numerical losses or approximations incurred by making this choice.

It pains us to point out that the PSF, as it is usually conceived, is
actually \emph{correlated} not \emph{convolved} with the true scene.
That is a choice about what the PSF is.  In what follows we will use
the word ``convolve'' in conformity with usual practice in astronomy,
but the way we show the PSFs in the figures, it is probably
``correlate'' that we are really doing.

To make our ideas about imaging concrete, we can represent the model
that will be used throughout this paper for imaging data as a
convolution
\begin{eqnarray}\displaystyle
\data_n &=& \psf_n \ast \scene + E_n
\quad ,
\end{eqnarray}
where $\data_n$ is the data image---one of $N$ noisy $M$-pixel
images---with index $1<n<N$, $\psf_n$ is the pixel-convolved PSF
appropriate for image $n$, $\ast$ represents the convolution
operation, $\scene$ is the ``true'' intensity field (``scene'') above
the atmosphere, and $E_n$ is the noise contribution to image $n$.
Convolution is a linear operation, so, following \citet{hirsch}, we
can write the model in two equivalent ways:
\begin{eqnarray}\displaystyle
d_n &=& P_n \cdot s + e_n
\\
d_n &=& S \cdot p_n + e_n
\quad ,
\end{eqnarray}
where now $d_n$ is the original data image $\data_n$ reformatted as a
one-dimensional length-$M$ column vector, $P_n$ is a $W\times M$
sparse matrix that contains $K$ independent values representing the
point-spread function for image $n$ on a frighteningly reformatted
pixel grid, $s$ is a length-$W$ column vector representing the true
scene, $e_n$ is a length-$M$ column vector of noise constributions to
image $n$, $\scene$ is a $K\times M$ sparse matrix that contains $W$
independent values representing the true scene also frighteningly
reformatted, and $p_n$ is a length-$K$ column vector representing the
PSF.  The idea is that if the image data are unwrapped into a
one-dimensional vector, the linear convolution operation can always be
represented as a matrix operation acting on another vector, and there
is a choice of whether to see the PSF as the matrix and the scene as
the vector, or vice versa.  Not obvious?  Think about it!

...insert here a figure demonstrating the model for one of the \LCOGT\ images...

In real astronomical applications, neither the PSF nor the true scene
is known \foreign{a priori}.  If our interest is the true scene---and
it usually is---then astronomy is a special case of what's known in
the computer science literature as \emph{blind deconvolution}.  We
want to explain the data as being produced by something fundamental of
interest convolved with instrumental resolution that is not known in
advance and also of no particular interest in itself.  Astronomers are
loath to use the word ``deconvolution'' but any time we make a catalog
of precise stellar positions and fluxes from noisy, low-resolution
data, we are performing some kind of deconvolution: We have
information about the positions of stars that is much more precise
than the the angular precision of any instrument-realized PSF.

One small but perhaps not insignificant issue with the model expressed
here is that it posits the existence of a ``true scene''.  For
technical reasons---related to the finiteness of any real data
stream---it is better to think of this ``true image'' as really an
instrument for making finite-resolution predictions.  It is not going
to be an accurate representation of the scene we would see with an
implausible infinitely large telescope!  It is a representation of the
intensity field that is only to be used within the context of
covolution with a finite PSF.  Similar issues arise in radio astronomy
when interferometric data are ``cleaned''.

Now, with an error model---a probabilistic description of how the
$E_n$ image (or equivalently, $e_n$ vector) is generated---we can
write down a likelihood function or a probability for the data $\data_n$
(or equivalently $d_n$) given the model parameters $p_n$ and $s$.  For
example, if the $M$ per-pixel noise contributions are (independently)
drawn from Gaussians with zero means then this likelihood has the
incredibly simple form
\begin{eqnarray}\displaystyle
\ln p(\data_n\given p_n, s) &=& Q - \frac{1}{2}\,\chi^2_n
\\
\chi^2_n &\equiv& \transpose{[d_n - P_n \cdot s]} \cdot C^{-1} \cdot [d_n - P_n \cdot s]
\quad ,
\end{eqnarray}
where $Q$ is some constant, we have used the column-vectoriness of
$d_n$, sparse matrix $P_n$ is trivially constructable from $p_n$, and
$C^{-1}$ is a diagonal $M\times M$ matrix with per-pixel inverse
variances on the diagonal and zeros everywhere else; this gives
$\chi^2_n$ the standard ``chi-squared'' meaning.  (The data pixels can
be made non-independent by adding off-diagonal terms to $C^{-1}$.)

With some priors, we can even in principle write down a posterior
probability distribution function over true scenes $s$ and PSFs $p_n$
given the data (the set of all $N$ images $\data_n$ or vectors $d_n$).
This posterior PDF can even be \emph{marginalized} over all possible
PSFs to leave us with a marginalized posterior PDF for just the true
scene $s$.  That would be just about the best we could possibly do in
this problem, for very general reasons.

Unfortunately, even if we apply unrealistically restrictive priors on
image space, any posterior PDF over true scenes $s$ will be a
dimensionally immense object.  Even when, in the experiments that
follow, we restrict to small image patches, any useful description of
``scene space'' will have a dimensionality $>10^4$ (the number of
pixels in the scene representation).  Perhaps someday soon we will
have good ways of describing non-trivial PDFs in spaces like
this---maybe we even already do---but we (the authors) don't know
anything realistic at the present day.  So proper probabilistic
inference is not an option in the short term.  The posterior PDF over
PSFs is even worse, because there is a different PSF for every image,
and in the experiments that follow we will be using $>10^3$ images,
each of which has a $10^{2.5}$-parameter PSF.

These considerations lead us to optimization rather than sampling or
full probabilistic approaches.  What to optimize?

...How regularization and optimization is related to the prior PDF and posterior PDF...

...Some general discussion of regularizations and their consequences.
Spend some lines bashing non-negative on $s$.  Why is that so
dangerous?..

...How, in general, are we going to proceed; why is online useful...

\section{The method and pipeline}

The method is, for each of $N$ data images $\data_n$ taken in turn:

1. Use the current scene image $\scene$ to infer the PSF $\psf_n$ for data
image $\data_n$ by non-negative least squares.  The PSF model at this
stage is a fixed-grid mixture of $\sigma=1$~pix ($FWHM = 2.35$~pix)
two-dimensional, circular Gaussians.  In detail, when we infer the
PSF, we also infer a sky level, and apply a L2 regularization to keep
the PSF smooth when there are degeneracies.

2. ``Deconvolve'' the PSF $\psf_n$ to make the deconvolved PSF
$\dpsf_n$. We perform this deconvolution by replacing the mixture of
Gaussians with a mixture of delta-functions, with the same amplitudes.
This follows the general idea of band-limited deconvolution
\citep{magain}.

3. Infer the scene $\scene_n$ implied by this data image $\data_n$ given the
deconvolved PSF $\dpsf_n$.  This fit is done without constraints
except a small regularization to deal with the fact that the scene is
bigger than the data image.

4. Update the scene by combining the current scene $\scene$ with the newly
inferred scene $\scene_n$ according to
\begin{eqnarray}\displaystyle
S &\leftarrow& [1-\alpha_n]\,S + \alpha_n\,S_n
\quad ,
\end{eqnarray}
where $\alpha_n$ is either $2/n$ or else $2/N$.  Possibly also apply
non-negative clipping, sky adjustment, or other constraints or
regularizations.

\section{Experiments and results}

\section{Discussion}

\acknowledgements It is a pleasure to thank
  Adam Bolton (Utah),
  Brendon Brewer (UCSB),
  Stefan Harmeling (T\"ubingen),
  Michael Hirsch (UCL),
  Phil Marshall (Oxford), and
  Bernhard Sch\"olkopf (T\"ubingen)
for contributions and comments.  We
also single out Dustin Lang (CMU) and Robert Lupton (Princeton) for
special thanks; DWH wouldn't understand anything about imaging if it
weren't for the many years of discussions.  DWH and DFM were partially
supported by NASA (grant GRANT NUMBER HERE) and the NSF (grant
IIS-1124794).  This project made use of the NASA \project{Astrophysics
  Data System}, and code in the the \project{numpy}, \project{scipy},
and \project{matplotlib} open-source projects.  All the code used in
this paper is available at [URL HERE], and sample data are available
at [URL HERE].

\begin{thebibliography}{70}
\bibitem[Hirsch \etal(2011)]{hirsch}
Hirsch,~M., Harmeling,~S., Sra,~S., Sch\"olkopf,~B., 2011, \aap, 531, A9
\bibitem[Magain \etal(1998)]{magain}
Magain,~P., Courbin,~F., Sohy,~S., 1998, \apj, 494, 472 
\end{thebibliography}

\end{document}
